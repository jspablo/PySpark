{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing course assignments\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import asc, lit, udf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conf = SparkConf().setAppName('Assignment').setMaster('local')\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing `SparkSession`, it is a combination of the previous versions `SparkConf`, `SparkContext`, `SQLContext`, and `HiveContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1.\n",
    "#### Leer el archivo data/containers.csv y contar el número de líneas.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo los datos del archivo csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mAssignments.ipynb\u001b[0m*  \u001b[34;42mdata_processing_course-master\u001b[0m/  \u001b[34;42mspark-warehouse\u001b[0m/\r\n",
      "\u001b[01;32mcontainers.csv\u001b[0m*     \u001b[01;32mPySpark.ipynb\u001b[0m*                  \u001b[34;42mtextMining\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/jovyan/work/data_processing_course-master/assignments/data/containers.csv\"\n",
    "csvdata = spark.read.format(\"csv\").option(\"header\", \"false\").load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='ship_imo;ship_name;country;departure;container_id;container_type;container_group;net_weight;gross_weight;owner;declared;contact;customs_ok'),\n",
       " Row(_c0='AMC1861710;Jayden;BD;201602183;FCUK1755843;4960;28VH;44804866.62;2240243.33;Streich-Wilkinson;Music')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobando obtención de los datos.\n",
    "csvdata.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contando el número de líneas del archivo csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvdata.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2.\n",
    "#### Leer el archivo data/containers.csv y filtrar aquellos contenedores cuyo ship_imo es DEJ1128330 y el grupo del contenedor es 22P1. Guardar los resultados en un archivo de texto en resultados/resutado_2.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['ship_imo',\n",
       "  'ship_name',\n",
       "  'country',\n",
       "  'departure',\n",
       "  'container_id',\n",
       "  'container_type',\n",
       "  'container_group',\n",
       "  'net_weight',\n",
       "  'gross_weight',\n",
       "  'owner',\n",
       "  'declared',\n",
       "  'contact',\n",
       "  'customs_ok'],\n",
       " ['AMC1861710',\n",
       "  'Jayden',\n",
       "  'BD',\n",
       "  '201602183',\n",
       "  'FCUK1755843',\n",
       "  '4960',\n",
       "  '28VH',\n",
       "  '44804866.62',\n",
       "  '2240243.33',\n",
       "  'Streich-Wilkinson',\n",
       "  'Music']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(csvdata))\n",
    "datardd = csvdata.rdd\n",
    "print(type(datardd))\n",
    "\n",
    "datasplit = datardd.map(lambda row: row[0].split(\";\"))\n",
    "\n",
    "datasplit.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ship_imo',\n",
       " 'AMC1861710',\n",
       " 'POG1615575',\n",
       " 'SQH1155999',\n",
       " 'JCI1797526',\n",
       " 'MBV1836745',\n",
       " 'GYR1192020',\n",
       " 'GLV1922612',\n",
       " 'NLH1771681',\n",
       " 'FUS1202266']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datafiltered = datasplit.filter(lambda row: row == '22P1')\n",
    "datasplit.map(lambda row: row[0]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasplit.filter(lambda row: row[6] == '22P1' and row[0] == 'DEJ1128330').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DEJ1128330',\n",
       "  'Tiara',\n",
       "  'GP',\n",
       "  '2016021818',\n",
       "  'GYFD1228113',\n",
       "  '20PF',\n",
       "  '22P1',\n",
       "  '51503716.88',\n",
       "  '5150371.69',\n",
       "  'Armstrong-Goldner',\n",
       "  'Automotive'],\n",
       " ['DEJ1128330',\n",
       "  'Tiara',\n",
       "  'GP',\n",
       "  '2016021818',\n",
       "  'MBPF1909627',\n",
       "  '24H2',\n",
       "  '22P1',\n",
       "  '37266600.88',\n",
       "  '1863330.04',\n",
       "  'Lehner-Hamill',\n",
       "  'Jewelery']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasplit.filter(lambda row: row[6] == '22P1' and row[0] == 'DEJ1128330').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardando los datos como archivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = datasplit.filter(lambda row: row[6] == '22P1' and row[0] == 'DEJ1128330')\n",
    "x1.saveAsTextFile('data_processing_course-master/assignments/resultados/resultado_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3.\n",
    "#### Leer el archivo data/containers.csv y convertir a formato Parquet. Recuerda que puedes hacer uso de la funcion parse_container en helpers.py tal y como vimos en clase. Guarda los resultados en resultados/resultado_3.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `SparkSession` to create a Dataframe from a csv. I am using a Dataframe instead of a RDD, because are RDDs are schemaless, and Dataframes are close to Parquet files structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data3 = spark.read.csv(path, header=True, sep=';') #Usamos el mismo path creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------+----------+------------+--------------+---------------+-----------+------------+-----------------+--------------------+--------------------+----------+\n",
      "|  ship_imo|        ship_name|country| departure|container_id|container_type|container_group| net_weight|gross_weight|            owner|            declared|             contact|customs_ok|\n",
      "+----------+-----------------+-------+----------+------------+--------------+---------------+-----------+------------+-----------------+--------------------+--------------------+----------+\n",
      "|AMC1861710|           Jayden|     BD| 201602183| FCUK1755843|          4960|           28VH|44804866.62|  2240243.33|Streich-Wilkinson|Music, Tools, Aut...|octavia@stammbedn...|      true|\n",
      "|POG1615575|Lake Eribertoland|     CR|2016021611| PDXW1549639|          28VH|           8888|16681047.32|   500431.42|  Senger and Sons|   Movies & Jewelery|cindy.dubuque@rob...|      true|\n",
      "|SQH1155999|           Aileen|     GN| 201602191| PLKO1661930|          L2R3|           L5R1|69102632.38|  3455131.62| Thompson-Kautzer|   Movies & Jewelery|otha@granteichman...|      true|\n",
      "+----------+-----------------+-------+----------+------------+--------------+---------------+-----------+------------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data_processing_course-master/assignments/resultados\n"
     ]
    }
   ],
   "source": [
    "cd data_processing_course-master/assignments/resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data3.write.parquet(\"data_processing_course-master/assignments/resultados/resultado_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4.\n",
    "#### Lee el archivo de Parquet guardado en el ejercicio 3 y filtra los barcos que tienen al menos un contenedor donde la columna customs_ok es igual a false. Extrae una lista con los identificadores de barco, ship_imo, sin duplicados y ordenados alfabéticamente, en formato json.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data4 = spark.read.parquet(\"data_processing_course-master/assignments/resultados/resultado_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ship_imo',\n",
       " 'ship_name',\n",
       " 'country',\n",
       " 'departure',\n",
       " 'container_id',\n",
       " 'container_type',\n",
       " 'container_group',\n",
       " 'net_weight',\n",
       " 'gross_weight',\n",
       " 'owner',\n",
       " 'declared',\n",
       " 'contact',\n",
       " 'customs_ok']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data4.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  ship_imo|\n",
      "+----------+\n",
      "|KSP1096387|\n",
      "|GYR1192020|\n",
      "|JET1053895|\n",
      "|SQH1155999|\n",
      "|NLH1771681|\n",
      "|JCI1797526|\n",
      "|GEU1548633|\n",
      "|AEY1108363|\n",
      "|IWE1254579|\n",
      "|AMC1861710|\n",
      "|POG1615575|\n",
      "|MBV1836745|\n",
      "|GLV1922612|\n",
      "|YZX1455509|\n",
      "|TCU1641123|\n",
      "|JMP1637582|\n",
      "|DEJ1128330|\n",
      "|RYP1117603|\n",
      "|FUS1202266|\n",
      "|NCZ1777367|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT ship_imo FROM data WHERE customs_ok = 'false'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data4 = spark.sql(\"SELECT DISTINCT ship_imo FROM data WHERE customs_ok = 'false'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT ship_imo FROM data WHERE customs_ok = 'false'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ship_imo='AEY1108363'),\n",
       " Row(ship_imo='AMC1861710'),\n",
       " Row(ship_imo='DEJ1128330'),\n",
       " Row(ship_imo='FUS1202266'),\n",
       " Row(ship_imo='GEU1548633'),\n",
       " Row(ship_imo='GLV1922612'),\n",
       " Row(ship_imo='GYR1192020'),\n",
       " Row(ship_imo='IWE1254579'),\n",
       " Row(ship_imo='JCI1797526'),\n",
       " Row(ship_imo='JET1053895'),\n",
       " Row(ship_imo='JMP1637582'),\n",
       " Row(ship_imo='KSP1096387'),\n",
       " Row(ship_imo='MBV1836745'),\n",
       " Row(ship_imo='NCZ1777367'),\n",
       " Row(ship_imo='NLH1771681'),\n",
       " Row(ship_imo='POG1615575'),\n",
       " Row(ship_imo='RYP1117603'),\n",
       " Row(ship_imo='SQH1155999'),\n",
       " Row(ship_imo='TCU1641123'),\n",
       " Row(ship_imo='YZX1455509')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.sort(asc(\"ship_imo\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data4.sort(asc(\"ship_imo\")).write.json(\"data_processing_course-master/assignments/resultados/resultado_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5.\n",
    "#### Crea una UDF para validar el código de identificación del contenedor container_id. Para simplificar la validación, daremos como válidos aquellos códigos compuestos de 3 letras para el propietario, 1 letra para la categoría, 6 números y 1 dígito de control. Devuelve un DataFrame con los campos: ship_imo, container_id, propietario, categoria, numero_serie y digito_control.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|container_id|\n",
      "+------------+\n",
      "| DXTQ1407119|\n",
      "| VXNB1938296|\n",
      "| OVMU1118217|\n",
      "| EAQO1539643|\n",
      "| SGQH1799946|\n",
      "| SWXT1708984|\n",
      "| EDBR1562470|\n",
      "| FCMB1487245|\n",
      "| UBRI1681197|\n",
      "| LFTG1322014|\n",
      "| RAUX1713695|\n",
      "| QKOJ1756061|\n",
      "| JVFH1614514|\n",
      "| KETX1362337|\n",
      "| KUOG1927848|\n",
      "| GSVC1467358|\n",
      "| JXRI1226202|\n",
      "| QZKL1853985|\n",
      "| HDGM1122708|\n",
      "| ZBJH1066313|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT container_id FROM data \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting familiar with Python re package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(container_id='DXTQ1407119'), Row(container_id='VXNB1938296')]\n"
     ]
    }
   ],
   "source": [
    "test5 = spark.sql(\"SELECT DISTINCT container_id FROM data \").take(2)\n",
    "type(test5)\n",
    "type(test5[0])\n",
    "print(str(test5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "containeridre = re.search(r'\\w\\w\\w\\w\\d\\d\\d\\d\\d\\d\\d', str(test5[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ez pz\n"
     ]
    }
   ],
   "source": [
    "if containeridre:\n",
    "    print('ez pz')\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating function to validate container ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validateID(containerID):\n",
    "    containeridre = re.search(r'\\w\\w\\w\\w\\d\\d\\d\\d\\d\\d\\d', str(containerID))\n",
    "    #nonvalid = []\n",
    "    if containeridre:\n",
    "        return(\"Container ID valid\")\n",
    "    else:\n",
    "        #nonvalid.append(str(containerID))\n",
    "        return(\"----- Container ID not valid: \" + str(containerID) + '-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5 = spark.sql(\"SELECT DISTINCT container_id FROM data \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5col = data5.select(\"container_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(container_id='DXTQ1407119'), Row(container_id='VXNB1938296')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5col.rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Container ID valid'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validateID('DXTQ1407119')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " '----- Container ID not valid: Row(container_id=None)-----',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " \"----- Container ID not valid: Row(container_id='JMYG190Z978')-----\",\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " \"----- Container ID not valid: Row(container_id='GJFL14A2798')-----\",\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " \"----- Container ID not valid: Row(container_id='CTVU1506A832')-----\",\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " \"----- Container ID not valid: Row(container_id='DUKF166276')-----\",\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid',\n",
       " 'Container ID valid']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5.rdd.map(lambda row: validateID(str(row))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contenedores que no tienen un código válido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['----- Container ID not valid: Row(container_id=None)-----',\n",
       " \"----- Container ID not valid: Row(container_id='JMYG190Z978')-----\",\n",
       " \"----- Container ID not valid: Row(container_id='GJFL14A2798')-----\",\n",
       " \"----- Container ID not valid: Row(container_id='CTVU1506A832')-----\",\n",
       " \"----- Container ID not valid: Row(container_id='DUKF166276')-----\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5.rdd.map(lambda row: validateID(str(row))).filter(lambda x: str(x) != 'Container ID valid').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdata = spark.sql(\"SELECT DISTINCT container_id FROM data \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando un nuevo dataframe con las columnas deseadas y eliminando los contenedores no válidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-50-34155b3894c8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-34155b3894c8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    data6new.withColumn(\"ship_imo\", lambda \"ship_imo\": str(\"ship_imo\") + 'asd')\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data6new.withColumn(\"ship_imo\", lambda \"ship_imo\": str(\"ship_imo\") + 'asd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6. \n",
    "#### Extrae una lista con peso total de cada barco, net_weight, sumando cada contenedor y agrupado por los campos ship_imo y container_group. Devuelve un DataFrame con la siguiente estructura: ship_imo, ship_name, container, total_net_weight.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6 = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ship_imo='AMC1861710', ship_name='Jayden', country='BD', departure='201602183', container_id='FCUK1755843', container_type='4960', container_group='28VH', net_weight='44804866.62', gross_weight='2240243.33', owner='Streich-Wilkinson', declared='Music, Tools, Automotive & Health', contact='octavia@stammbednar.name', customs_ok='true'),\n",
       " Row(ship_imo='POG1615575', ship_name='Lake Eribertoland', country='CR', departure='2016021611', container_id='PDXW1549639', container_type='28VH', container_group='8888', net_weight='16681047.32', gross_weight='500431.42', owner='Senger and Sons', declared='Movies & Jewelery', contact='cindy.dubuque@roberts.org', customs_ok='true')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6new = data6.drop(\"country\", \"departure\", \"container_type\", \\\n",
    "                      \"container_group\", \"gross_weight\", \"owner\", \\\n",
    "                      \"declared\", \"customs_ok\", \"contact\", \"net_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------+\n",
      "|  ship_imo|         ship_name|container_id|\n",
      "+----------+------------------+------------+\n",
      "|AMC1861710|            Jayden| FCUK1755843|\n",
      "|POG1615575| Lake Eribertoland| PDXW1549639|\n",
      "|SQH1155999|            Aileen| PLKO1661930|\n",
      "|JCI1797526|          Herminio| BXMT1827488|\n",
      "|MBV1836745|Port Guiseppeburgh| JYIE1892741|\n",
      "|GYR1192020|         Emardland| LARQ1499256|\n",
      "|GLV1922612|           Eulalia| ARDX1463154|\n",
      "|NLH1771681|       Port Noemie| JFPX1246669|\n",
      "|FUS1202266|  East Mustafaland| ICAV1235470|\n",
      "|GLV1922612|           Eulalia| KEVU1145768|\n",
      "|IWE1254579|      North Creola| VDUQ1801278|\n",
      "|JET1053895|             Jamil| CXZN1286843|\n",
      "|KSP1096387|             Wiley| YAZN1142572|\n",
      "|GYR1192020|         Emardland| HVFU1799048|\n",
      "|GYR1192020|         Emardland| ROML1055099|\n",
      "|JMP1637582|East Zechariahland| LONM1299749|\n",
      "|TCU1641123|     New Margarete| XKAO1357085|\n",
      "|MBV1836745|Port Guiseppeburgh| JYPA1889923|\n",
      "|POG1615575| Lake Eribertoland| GKXC1181753|\n",
      "|AEY1108363|             Juana| GJFL14A2798|\n",
      "+----------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data6new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6new = data6new.withColumn(\"total_net_weight\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         ship_name|\n",
      "+------------------+\n",
      "|         Emardland|\n",
      "|             Tiara|\n",
      "|East Zechariahland|\n",
      "| Lake Eribertoland|\n",
      "|             Jamil|\n",
      "|           Eulalia|\n",
      "|  East Mustafaland|\n",
      "|   New Santinoberg|\n",
      "|            Aileen|\n",
      "|            Jayden|\n",
      "|        Marksshire|\n",
      "|      North Creola|\n",
      "|       Port Noemie|\n",
      "|     New Margarete|\n",
      "|             Wiley|\n",
      "|   East Savanahton|\n",
      "|          Herminio|\n",
      "|Port Guiseppeburgh|\n",
      "|           Lomaton|\n",
      "|             Juana|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT ship_name FROM data\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mismo filtrado de datos en dos formas distintas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT container_id FROM data WHERE ship_name='Emardland' \").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6.filter(data6['ship_name']=='Emardland').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = data6.select(\"gross_weight\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2 = data6.select(\"net_weight\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumtwocolumns(x1,x2):\n",
    "    if len(x1) != len(x2):\n",
    "        return 'Lists have different lengths'\n",
    "    else:\n",
    "        for i in range(len(x1)):\n",
    "            return x1[i]+x2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'register'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-48098ee84096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mudffunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumtwocolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'register'"
     ]
    }
   ],
   "source": [
    "udffunction = udf.register(sumtwocolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = data6new.rdd.filter(\"ship_imo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 1 times, most recent failure: Lost task 0.0 in stage 84.0 (TID 2302, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-abce4c5eba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 1 times, most recent failure: Lost task 0.0 in stage 84.0 (TID 2302, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "z.map(lambda x: x).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 7.\n",
    "#### Guarda los resultados del ejercicio anterior en formato Parquet.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 8.\n",
    "#### ¿En qué casos crees que es más eficiente utilizar formatos como Parquet? ¿Existe alguna desventaja frente a formatos de texto como CSV?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Los formatos columnares facilitan el procesamiento, lectura y escritura de los motores como Spark, siendo hasta 10 veces más rápidos en leer los mismos datos almacenados en csv.\n",
    "\n",
    "Un ejemplo práctico en el que observar las ventajas del archivos como parquet es el caso de Amazon Athenea, el cual es un servicio para realizar consultas SQL. ['Ejemplo de precios'](https://aws.amazon.com/es/athena/pricing/)\n",
    "\n",
    "En el anterior enlace se explica como la misma consulta realizada sobre archivos parquet frente a csv permite reducir el coste de una consulta de 15€(sobre csv) a 1.67€(sobre parquet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 9.\n",
    "#### ¿Es posible procesar XML mediante Spark? ¿Existe alguna restricción por la cual no sea eficiente procesar un único archivo en multiples nodos? ¿Se te ocurre alguna posible solución para trocear archivos suficientemente grandes? ¿Existe la misma problemática con otros formatos de texto como JSON?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sería posible mediante un librería externa (spark.read.xml que viene de databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 10.\n",
    "#### Spark SQL tiene una función denominada avg que se utiliza para calcular el promedio de un conjunto de valores ¿Por qué los autores han creado esta función en lugar de usar el API estándar de Python o Scala?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
