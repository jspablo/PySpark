{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running PySpark on docker by using [jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook/) container. This notebook shows test scripts working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Assignment').setMaster('local')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mdata_processing_course-master\u001b[0m/  \u001b[01;32mPySpark.ipynb\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data_processing_course-master\n"
     ]
    }
   ],
   "source": [
    "cd data_processing_course-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42massignments\u001b[0m/  \u001b[01;32mbootstrap.sh\u001b[0m*  \u001b[34;42minfra\u001b[0m/    \u001b[01;32mlocal_setup.sh\u001b[0m*  \u001b[34;42mspark\u001b[0m/\r\n",
      "\u001b[34;42mbeam\u001b[0m/         \u001b[34;42mdata\u001b[0m/          \u001b[01;32mLICENSE\u001b[0m*  \u001b[01;32mREADME.md\u001b[0m*       \u001b[01;32mVagrantfile\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data_processing_course-master/assignments\n"
     ]
    }
   ],
   "source": [
    "cd assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbootstrap.sh\u001b[0m*     \u001b[01;32mpytest.ini\u001b[0m*           \u001b[01;32mtest_ejercicio_3.py\u001b[0m*\r\n",
      "\u001b[01;32mconftest.py\u001b[0m*      \u001b[01;32mREADME.md\u001b[0m*            \u001b[01;32mtest_ejercicio_4.py\u001b[0m*\r\n",
      "\u001b[01;32mcontenedores.py\u001b[0m*  \u001b[01;32mrequirements.txt\u001b[0m*     \u001b[01;32mtest_ejercicio_5.py\u001b[0m*\r\n",
      "\u001b[34;42mdata\u001b[0m/             \u001b[01;32mtest_ejercicio_0.py\u001b[0m*  \u001b[01;32mtest_ejercicio_6.py\u001b[0m*\r\n",
      "\u001b[01;32mhelpers.py\u001b[0m*       \u001b[01;32mtest_ejercicio_1.py\u001b[0m*  \u001b[01;32mVagrantfile\u001b[0m*\r\n",
      "\u001b[34;42m__pycache__\u001b[0m/      \u001b[01;32mtest_ejercicio_2.py\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run contenedores.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9\r\n"
     ]
    }
   ],
   "source": [
    "cat resultados/resultado_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the `SparkSession`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past, you would potentially work with `SparkConf`, `SparkContext`, `SQLContext`, and `HiveContext` to execute your various Spark queries for configuration, Spark context, SQL context, and Hive context respectively. The **`SparkSession`** is essentially the combination of these contexts including StreamingContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataframe from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SparkSession previously imported by: from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringJSONRDD = sc.parallelize((\"\"\"\n",
    "  { \"id\": \"123\",\n",
    "\"name\": \"Katie\",\n",
    "\"age\": 19,\n",
    "\"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "\"\"\"{\n",
    "\"id\": \"234\",\n",
    "\"name\": \"Michael\",\n",
    "\"age\": 22,\n",
    "\"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "\"\"\"{\n",
    "\"id\": \"345\",\n",
    "\"name\": \"Simone\",\n",
    "\"age\": 23,\n",
    "\"eyeColor\": \"blue\",\n",
    "\"randomkey\": \"Single value\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondata = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+------------+\n",
      "|age|eyeColor| id|   name|   randomkey|\n",
      "+---+--------+---+-------+------------+\n",
      "| 19|   brown|123|  Katie|        null|\n",
      "| 22|   green|234|Michael|        null|\n",
      "| 23|    blue|345| Simone|Single value|\n",
      "+---+--------+---+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsondata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataframe from csv file (We need to create first and RDD, split it into cells and transform it at the end in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'csvFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-569bc22ed8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsvdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsvFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'containers.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'csvFile'"
     ]
    }
   ],
   "source": [
    "csvdata = spark.read.csv('containers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|ship_imo;ship_nam...|\n",
      "|AMC1861710;Jayden...|\n",
      "|POG1615575;Lake E...|\n",
      "|SQH1155999;Aileen...|\n",
      "|JCI1797526;Hermin...|\n",
      "|MBV1836745;Port G...|\n",
      "|GYR1192020;Emardl...|\n",
      "|GLV1922612;Eulali...|\n",
      "|NLH1771681;Port N...|\n",
      "|FUS1202266;East M...|\n",
      "|GLV1922612;Eulali...|\n",
      "|IWE1254579;North ...|\n",
      "|JET1053895;Jamil;...|\n",
      "|KSP1096387;Wiley;...|\n",
      "|GYR1192020;Emardl...|\n",
      "|GYR1192020;Emardl...|\n",
      "|JMP1637582;East Z...|\n",
      "|TCU1641123;New Ma...|\n",
      "|MBV1836745;Port G...|\n",
      "|POG1615575;Lake E...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvdata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
